#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SumÃ¡rio: Sistema de Pipeline de Coleta de ImÃ³veis
Mostra o resultado do fluxo integrado busca_ampla â†’ DB â†’ scraper â†’ CSV
"""

from pathlib import Path
from datetime import datetime
import sqlite3
import csv

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            ğŸ  SISTEMA DE COLETA DE IMÃ“VEIS - SUMÃRIO FINAL ğŸ                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# 1. Arquivos gerados
output_dir = Path("output")
csv_files = sorted(output_dir.glob("imoveis_scraper_escalavel*.csv"), reverse=True)

if csv_files:
    latest_csv = csv_files[0]
    csv_size = latest_csv.stat().st_size / 1024  # em KB
    csv_mtime = datetime.fromtimestamp(latest_csv.stat().st_mtime)
    
    # Contar linhas
    with open(latest_csv, encoding='utf-8') as f:
        csv_rows = sum(1 for _ in f) - 1  # -1 para o header
    
    print(f"""
ğŸ“ ARQUIVOS GERADOS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  CSV EXPORTADO:
  â””â”€ {latest_csv.name}
     â””â”€ Tamanho: {csv_size:.1f} KB
     â””â”€ Linhas: {csv_rows}
     â””â”€ Criado: {csv_mtime.strftime('%Y-%m-%d %H:%M:%S')}
""")

# 2. Banco de dados (fixo na raiz do projeto)
db_path = Path(__file__).resolve().parent / "imoveis.db"
if db_path.exists():
    db_size = db_path.stat().st_size / 1024 / 1024  # em MB
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    cursor.execute('SELECT COUNT(*) FROM imoveis')
    total_imoveis = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM links')
    total_links = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(DISTINCT fonte) FROM imoveis')
    total_dominios = cursor.fetchone()[0]
    
    conn.close()
    
    print(f"""
ğŸ’¾ BANCO DE DADOS SQLITE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  Arquivo: imoveis.db ({db_size:.2f} MB)
  
  âœ“ ImÃ³veis coletados: {total_imoveis}
  âœ“ Links processados: {total_links}
  âœ“ DomÃ­nios Ãºnicos: {total_dominios}
  âœ“ Tabelas: imoveis, links, checkpoint
""")

# 3. Arquitetura do sistema
print("""
ğŸ”„ ARQUITETURA DO SISTEMA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  FASE 1: DESCOBERTA DE LINKS (busca_ampla.py)
  â”œâ”€ Entrada: Palavra-chave (ex: "apartamento sÃ£o paulo")
  â”œâ”€ MÃ©todo: Bing site: search via Selenium + undetected-chromedriver
  â”œâ”€ Features:
  â”‚  â”œâ”€ Click-follow para resolver redirecionamentos Bing
  â”‚  â”œâ”€ DetecÃ§Ã£o automÃ¡tica de CAPTCHA
  â”‚  â””â”€ Pausa para resoluÃ§Ã£o manual
  â””â”€ SaÃ­da: JSON com URLs reais (19 links coletados)
  
  FASE 2: INTEGRAÃ‡ÃƒO NO BANCO (integrar.py)
  â”œâ”€ Entrada: JSON de busca_ampla
  â”œâ”€ AÃ§Ã£o: Insere links no SQLite com status "pending"
  â””â”€ SaÃ­da: Banco de dados com metadados

  FASE 3: EXTRAÃ‡ÃƒO DE DADOS (scraper_escalavel.py)
  â”œâ”€ Entrada: Links no banco SQLite
  â”œâ”€ MÃ©todo: Processamento paralelo (3 workers)
  â”œâ”€ ExtraÃ§Ã£o:
  â”‚  â”œâ”€ JSON-LD (preÃ§o, endereÃ§o, telefone)
  â”‚  â”œâ”€ Meta tags (descriÃ§Ã£o)
  â”‚  â””â”€ Regex patterns (CEP, Ã¡rea, quartos)
  â””â”€ SaÃ­da: Dados estruturados no SQLite

  FASE 4: EXPORTAÃ‡ÃƒO (integrar.py + scraper_escalavel.py)
  â”œâ”€ Entrada: Dados no SQLite
  â”œâ”€ Formato: CSV com 15 colunas
  â””â”€ SaÃ­da: imoveis_scraper_escalavel_*.csv

  FASE 5: RELATÃ“RIO (stats.py)
  â”œâ”€ Entrada: Banco SQLite
  â””â”€ SaÃ­da: EstatÃ­sticas e cobertura de dados
""")

# 4. Fluxo de comando
print("""
âœ¨ FLUXO DE USO (SEM INTERVENÃ‡ÃƒO MANUAL)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  $ python integrar.py -w 3
  
  Isso faz automaticamente:
  1. LÃª Ãºltimo JSON de busca_ampla (gerado manualmente com CAPTCHA)
  2. Insere 19 links no banco SQLite
  3. Processa com 3 workers paralelos
  4. Extrai dados (preÃ§o, endereÃ§o, telefone, etc)
  5. Salva 22 imÃ³veis no banco
  6. Exporta CSV com estrutura completa

  â±ï¸ Tempo total: ~4 minutos para 19 links
""")

# 5. PrÃ³ximas melhorias
print("""
ğŸš€ PRÃ“XIMAS MELHORIAS PARA ESCALA (50k+ imÃ³veis)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  CURTO PRAZO:
  â”œâ”€ Processar mÃºltiplas palavras-chave sequencialmente
  â”œâ”€ Aumentar workers paralelos (para 5-10)
  â”œâ”€ Adicionar proxy rotation (evitar CAPTCHA)
  â””â”€ Melhorar regex patterns para mais campos

  MÃ‰DIO PRAZO:
  â”œâ”€ API fallback (SerpAPI para descoberta de links)
  â”œâ”€ Scheduled jobs (cron/tarefa agendada)
  â””â”€ Database indexing (otimizar queries)

  LONGO PRAZO:
  â”œâ”€ IntegraÃ§Ã£o com Data Warehouse (BigQuery/Redshift)
  â”œâ”€ Real-time updates (verificar preÃ§os diariamente)
  â”œâ”€ ML pipeline (classificaÃ§Ã£o, prediÃ§Ã£o de preÃ§os)
  â””â”€ REST API (expor dados coletados)
""")

print("""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Sistema operacional e pronto para escalar!
   CSV salvo em: output/imoveis_scraper_escalavel_*.csv
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
