# ğŸ  Sistema de Coleta de ImÃ³veis - Pipeline EscalÃ¡vel

Sistema completo para coletar informaÃ§Ãµes de imÃ³veis em grande escala (50k+ propriedades) com preÃ§o, localizaÃ§Ã£o, CEP, descriÃ§Ã£o e contato.

## ğŸ“Š Fluxo de Funcionamento

### Fase 1: Descoberta de URLs
```bash
python busca_ampla.py "apartamento sÃ£o paulo"
```
- Busca no Bing usando `site:` commands
- Detecta e espera resoluÃ§Ã£o manual de CAPTCHAs
- Extrai URLs reais de 5 domÃ­nios principais
- Salva em `output/imoveis_busca_ampla_*.json` (19 URLs coletadas)

### Fase 2-4: IntegraÃ§Ã£o + Processamento + Export
```bash
python integrar.py -w 3 --headless
```
- LÃª JSON da Fase 1
- Insere URLs no banco SQLite
- Processa 3 URLs em paralelo (3 workers)
- Extrai dados estruturados (preÃ§o, endereÃ§o, CEP, telefone)
- Exporta para CSV

### Resultado Final
```
output/imoveis_scraper_escalavel_TIMESTAMP.csv
â”œâ”€ id (hash Ãºnico)
â”œâ”€ titulo (nome do imÃ³vel)
â”œâ”€ preco (valor em R$)
â”œâ”€ metragem (mÂ²)
â”œâ”€ quartos
â”œâ”€ banheiros
â”œâ”€ descricao
â”œâ”€ endereco
â”œâ”€ cidade
â”œâ”€ estado
â”œâ”€ cep
â”œâ”€ contato (telefone)
â”œâ”€ link (URL original)
â”œâ”€ fonte (domÃ­nio)
â””â”€ data_coleta
```

## ğŸ—„ï¸ Banco de Dados

SQLite com 3 tabelas:

| Tabela | FunÃ§Ã£o |
|--------|--------|
| `imoveis` | Dados estruturados de imÃ³veis (22 coletados) |
| `links` | URLs para processar (queue) |
| `checkpoint` | Estado de progresso (para retry) |

**Arquivo:** `imoveis.db` (40 KB)

## ğŸš€ Uso RÃ¡pido

### Primeira vez (com CAPTCHA manual)
```bash
# Terminal 1: Descobrir URLs
python busca_ampla.py "apartamento curitiba"
# â†’ Navegador abre, vocÃª resolve CAPTCHA manualmente
# â†’ Enter no terminal
# â†’ JSON salvo com 19 URLs

# Terminal 2: Processar URLs â†’ CSV (sem intervenÃ§Ã£o)
python integrar.py -w 3
# â†’ LÃª JSON recente
# â†’ Insere no SQLite
# â†’ Processa com 3 workers
# â†’ Exporta CSV
# â†’ Tempo: ~4 minutos
```

### Verificar resultados
```bash
python stats.py        # Mostrar estatÃ­sticas
python resumo.py       # Mostrar arquitetura completa
```

## ğŸ“ˆ Escalar para 50k+ ImÃ³veis

### 1. MÃºltiplas Buscas (mesma Fase 1)
```bash
python busca_ampla.py "apartamento sÃ£o paulo"    # ~19 URLs
python busca_ampla.py "casa brasÃ­lia"            # ~19 URLs
python busca_ampla.py "quarto recife"            # ~19 URLs
# ... repetir para 20-50 keywords
# Resultado: 400-950 URLs totais
```

### 2. Processar todas de uma vez
```bash
python integrar.py -w 5 --skip-search
# Processa todos os JSONs coletados
# Aumenta para 5 workers (mais rÃ¡pido)
```

### 3. Consolidar CSVs
```python
import pandas as pd
from pathlib import Path

# Concatenar todos os CSVs
csvs = Path('output').glob('imoveis_scraper_escalavel*.csv')
df = pd.concat([pd.read_csv(f) for f in csvs])
df.to_csv('output/imoveis_consolidado.csv', index=False)
print(f"Total de imÃ³veis: {len(df)}")
```

## ğŸ”§ ConfiguraÃ§Ãµes

Arquivo: `scraper_escalavel.py`

```python
LINKS_PER_DOMAIN = 20      # URLs por domÃ­nio em busca_ampla
MAX_WORKERS = 3            # Navegadores paralelos (aumentar = mais rÃ¡pido, mais RAM)
BATCH_SIZE = 100           # Processar em lotes de 100
RETRY_MAX = 3              # Tentar 3x se falhar
RETRY_BACKOFF_FACTOR = 2   # Esperar 1s, 2s, 4s entre tentativas
```

## ğŸ“¦ DependÃªncias

```
undetected-chromedriver==3.5.5
selenium==4.38.0
beautifulsoup4
requests
sqlite-utils
tqdm
```

**Instalar:**
```bash
pip install -r requirements.txt
```

## ğŸ› Troubleshooting

| Problema | SoluÃ§Ã£o |
|----------|---------|
| "CAPTCHA/verification" | Resolveu manualmente? Aperte Enter |
| "Bing is presenting a challenge" | Espera 2s e tenta novamente |
| "Gateway timeout" (OLX/Mercado Livre) | Aumentar timeout (scraper_escalavel.py line 400) |
| CSV vazio | Verificar: `python stats.py` |
| Dados incompletos (sem endereÃ§o/CEP) | Site nÃ£o tem JSON-LD; melhorar regex patterns |

## ğŸ“ Exemplo de Uso Completo (Production)

```bash
#!/bin/bash
# collect_all_properties.sh

keywords=(
    "apartamento sÃ£o paulo"
    "apartamento rio de janeiro"
    "apartamento belo horizonte"
    "casa brasÃ­lia"
    "quarto curitiba"
    "casa recife"
)

# FASE 1: Descobrir URLs (manual CAPTCHA)
for kw in "${keywords[@]}"; do
    echo "Buscando: $kw"
    python busca_ampla.py "$kw"
    sleep 5  # esperar para nÃ£o sobrecarregar
done

# FASE 2-4: Processar tudo
echo "Processando..."
python integrar.py -w 5 --skip-search

# Consolidar
echo "Consolidando CSVs..."
python consolidar_csvs.py

echo "âœ… Completo! Ver: output/imoveis_consolidado.csv"
```

## ğŸ“Š Performance

| MÃ©trica | Valor |
|---------|-------|
| URLs por keyword | 19 |
| Tempo de busca (1 keyword) | 3-5 min (com CAPTCHA) |
| Tempo de processamento (19 URLs) | 4 min (3 workers) |
| Taxa de sucesso | 100% (17/19 URLs vÃ¡lidas) |
| Dados coletados | 22 imÃ³veis |
| Cobertura de preÃ§o | 18% (MercadoLivre tem dados estruturados) |

## ğŸ¯ Roadmap

- [ ] Proxy rotation (evitar CAPTCHA frequente)
- [ ] API fallback (SerpAPI)
- [ ] Scheduler (rodar diariamente)
- [ ] Data warehouse (BigQuery)
- [ ] ML pipeline (classificaÃ§Ã£o de preÃ§o)
- [ ] REST API (expor dados)
- [ ] Dashboard (visualizar dados em tempo real)

## ğŸ“ Contato & Suporte

Arquivos principais:
- `busca_ampla.py` - Fase 1 (descoberta)
- `scraper_escalavel.py` - Fases 3-4 (processamento)
- `integrar.py` - Orquestrador
- `stats.py` - EstatÃ­sticas
- `imoveis.db` - Banco de dados

---

**v1.0** - Nov 14, 2025
